{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import TKinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Creating Universe and its UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World constants:\n",
    "UNIVERSE = [\n",
    "    ['S','0','0','0','0'],\n",
    "    ['W','W','W','W','0'],\n",
    "    ['0','0','0','W','0'],\n",
    "    ['0','W','0','0','0'],\n",
    "    ['0','W','W','W','W'],\n",
    "    ['0','0','0','0','G'],\n",
    "]\n",
    "\n",
    "# UNIVERSE = [\n",
    "#     ['S','0','0','0',],\n",
    "#     ['W','W','0','W',],\n",
    "#     ['0','0','0','W',],\n",
    "#     ['0','W','0','G',],\n",
    "# ]\n",
    "UNIT = 60 # size of each cell\n",
    "ACTION_SPACE = ['U','R','D','L','UR','UL','DR','DL','NO'] # actions (you can remove each one you want)\n",
    "UNIVERSE_SLEEP = 0   # game delay\n",
    "REWARD_GOAL = 30   # reward of reaching the goal\n",
    "REWARD_WALL = -20   # reward of falling into trap\n",
    "REWARD_EMPTY = 0   # reward of going to empty cell\n",
    "\n",
    "\n",
    "# UI class\n",
    "class Maze(tk.Tk,object):\n",
    "    def __init__(self):\n",
    "        super(Maze,self).__init__()\n",
    "        self.n_action = len(ACTION_SPACE)   #  Number of actions\n",
    "        self.n_rows = len(UNIVERSE)  # Number of rows\n",
    "        self.n_columns = len(UNIVERSE[0])   # Number of columns\n",
    "        self.title('Maze')  # title of program\n",
    "        self.geometry(f'{self.n_columns*UNIT}x{self.n_rows*UNIT}')\n",
    "        self.origin = np.array([UNIT/2,UNIT/2])\n",
    "        self._build_maze()\n",
    "    \n",
    "\n",
    "    # ----------- build UI from given universe ----------------\n",
    "    def _build_maze(self):\n",
    "        self.canvas = tk.Canvas(self,bg='white',\n",
    "                                height=self.n_rows*UNIT,\n",
    "                                width= self.n_columns*UNIT,\n",
    "                                )\n",
    "        \n",
    "\n",
    "        # creating grids\n",
    "        for i in range(0,self.n_rows*UNIT,UNIT):\n",
    "            x0,y0,x1,y1 = 0,i,self.n_columns*UNIT,i\n",
    "            self.canvas.create_line(x0,y0,x1,y1)\n",
    "        for i in range(0,self.n_columns*UNIT, UNIT):\n",
    "            x0,y0,x1,y1 = i,0,i,self.n_rows*UNIT\n",
    "            self.canvas.create_line(x0,y0,x1,y1)\n",
    "        \n",
    "\n",
    "        self.walls = []\n",
    "        padding = UNIT/2 - 5\n",
    "        for i in range(self.n_rows):\n",
    "            for j in range(self.n_columns):\n",
    "                dst = np.array([j*UNIT,i*UNIT])\n",
    "                if UNIVERSE[i][j] == 'W':\n",
    "                    wall = self.canvas.create_rectangle(self.origin[0] + dst[0] - padding,\n",
    "                                                        self.origin[1] + dst[1] - padding,\n",
    "                                                        self.origin[0] + dst[0] + padding,\n",
    "                                                        self.origin[1] + dst[1] + padding,\n",
    "                                                        fill = 'red',\n",
    "                                                        )\n",
    "                    self.walls.append(wall)\n",
    "                if UNIVERSE[i][j] == 'G':\n",
    "                    self.goal = self.canvas.create_oval(self.origin[0] + dst[0] - padding,\n",
    "                                                        self.origin[1] + dst[1] - padding,\n",
    "                                                        self.origin[0] + dst[0] + padding,\n",
    "                                                        self.origin[1] + dst[1] + padding,\n",
    "                                                        fill = 'green',\n",
    "                                                        )\n",
    "                if UNIVERSE[i][j] == 'S':\n",
    "                    self.start_point = dst\n",
    "        \n",
    "        self.marble = self.canvas.create_rectangle(self.origin[0] + self.start_point[0] - padding,\n",
    "                                                              self.origin[1] + self.start_point[1] - padding,\n",
    "                                                              self.origin[0] + self.start_point[0] + padding,\n",
    "                                                              self.origin[1] + self.start_point[1] + padding,\n",
    "                                                              fill = 'blue',\n",
    "                                                              )\n",
    "        \n",
    "        self.canvas.pack()\n",
    "\n",
    "\n",
    "    # ----------- reset universe --------------------------\n",
    "    def reset(self):\n",
    "        padding = UNIT/2 - 5\n",
    "        self.update()\n",
    "        time.sleep(UNIVERSE_SLEEP)\n",
    "        self.canvas.delete(self.marble)\n",
    "        self.marble = self.canvas.create_rectangle(self.origin[0] + self.start_point[0] - padding,\n",
    "                                                    self.origin[1] + self.start_point[1] - padding,\n",
    "                                                    self.origin[0] + self.start_point[0] + padding,\n",
    "                                                    self.origin[1] + self.start_point[1] + padding,\n",
    "                                                    fill = 'blue',\n",
    "                                                    )\n",
    "        \n",
    "        return self.canvas.coords(self.marble)\n",
    "    \n",
    "\n",
    "    def step(self,action):\n",
    "        s = self.canvas.coords(self.marble)\n",
    "        movement = np.array([0,0])\n",
    "\n",
    "        if action == 'U':\n",
    "            if s[1] > UNIT:\n",
    "                movement[1] -= UNIT\n",
    "        elif action == 'R':\n",
    "            if s[0] < (self.n_columns - 1) * UNIT:\n",
    "                movement[0] += UNIT\n",
    "        elif action == 'D':\n",
    "            if s[1] < (self.n_rows - 1) * UNIT:\n",
    "                movement[1] += UNIT\n",
    "        elif action == 'L':\n",
    "            if s[0] > UNIT:\n",
    "                movement[0] -= UNIT\n",
    "        elif action == 'UR':\n",
    "            if s[1] > UNIT:\n",
    "                movement[1] -= UNIT\n",
    "            if s[0] < (self.n_columns - 1) * UNIT:\n",
    "                movement[0] += UNIT\n",
    "        elif action == 'DR':\n",
    "            if s[0] < (self.n_columns - 1) * UNIT:\n",
    "                movement[0] += UNIT\n",
    "            if s[1] < (self.n_rows - 1) * UNIT:\n",
    "                movement[1] += UNIT\n",
    "        elif action == 'DL':\n",
    "            if s[1] < (self.n_rows - 1) * UNIT:\n",
    "                movement[1] += UNIT\n",
    "            if s[0] > UNIT:\n",
    "                movement[0] -= UNIT\n",
    "        elif action == 'UL':\n",
    "            if s[0] > UNIT:\n",
    "                movement[0] -= UNIT\n",
    "            if s[1] > UNIT:\n",
    "                movement[1] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.marble,movement[0],movement[1])\n",
    "        s_next = self.canvas.coords(self.marble)\n",
    "\n",
    "\n",
    "        # --------------- reward function -------------\n",
    "        # reaching goal\n",
    "        if s_next == self.canvas.coords(self.goal):\n",
    "            reward = REWARD_GOAL\n",
    "            done = True\n",
    "        # falling into trap\n",
    "        elif s_next in [self.canvas.coords(wall) for wall in self.walls]:\n",
    "            reward = REWARD_WALL\n",
    "            done = False\n",
    "        else:\n",
    "            reward = REWARD_EMPTY\n",
    "            done = False\n",
    "        \n",
    "        return s_next,reward,done\n",
    "    \n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        self.update()\n",
    "        time.sleep(UNIVERSE_SLEEP)\n",
    "\n",
    "    def goal_coords(self):\n",
    "        return self.canvas.coords(self.goal)\n",
    "\n",
    "    def walls_coords(self):\n",
    "        return [self.canvas.coords(wall) for wall in self.walls]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL constants:\n",
    "\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.8\n",
    "ALPHA = 0.1\n",
    "EPISODE = 1000\n",
    "\n",
    "class RL:\n",
    "    def __init__(self,goal,walls):\n",
    "        self.q_table = pd.DataFrame(columns=ACTION_SPACE)\n",
    "        self.goal = goal\n",
    "        self.walls = walls\n",
    "    \n",
    "    def check_state_exist(self,state):\n",
    "        if not state in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0.0]*len(ACTION_SPACE),\n",
    "                    name = state,\n",
    "                    index = self.q_table.columns\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    def choose_action(self,observation):\n",
    "        self.check_state_exist(observation)\n",
    "        if np.random.rand() < EPSILON:\n",
    "            actions = self.q_table.loc[observation,:]\n",
    "            actions = actions.reindex(np.random.permutation(actions.index))\n",
    "            return actions.idxmax()\n",
    "        return np.random.choice(ACTION_SPACE)\n",
    "    \n",
    "    def certain_action(self,observation):\n",
    "        actions = self.q_table.loc[observation,:]\n",
    "        actions = actions.reindex(np.random.permutation(actions.index))\n",
    "        return actions.idxmax()\n",
    "\n",
    "    def q_learn(self,state,action,reward,next_state):\n",
    "        self.check_state_exist(next_state)\n",
    "        \n",
    "        if next_state == self.goal:\n",
    "            self.q_table.loc[state,action] += ALPHA*(reward - self.q_table.loc[state,action])\n",
    "        else:\n",
    "            self.q_table.loc[state,action] += ALPHA*(reward + GAMMA*self.q_table.loc[next_state,:].max() - self.q_table.loc[state,action])\n",
    "    \n",
    "    def sarsa_learn(self,state,action,reward,next_state,next_action):\n",
    "        self.check_state_exist(next_state)\n",
    "\n",
    "        if next_state == self.goal:\n",
    "            self.q_table.loc[state,action] += ALPHA*(reward - self.q_table.loc[state,action])\n",
    "        else:\n",
    "            self.q_table.loc[state,action] += ALPHA*(reward + GAMMA*self.q_table.loc[next_state,next_action] - self.q_table.loc[state,action])\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_with_q_learing():\n",
    "    global UNIVERSE_SLEEP\n",
    "    m = Maze()\n",
    "    agent = RL(m.goal_coords(),m.walls_coords())\n",
    "\n",
    "    m.title('Maze - Training')\n",
    "\n",
    "\n",
    "    for i in range(EPISODE):\n",
    "        state = m.reset()\n",
    "        state = str(state)\n",
    "        step_counter = 0\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            clear_output(wait=True)\n",
    "            print(f'action : {action}')\n",
    "            print(agent.q_table)\n",
    "            m.render()\n",
    "            next_state,reward,done = m.step(action)\n",
    "            next_state = str(next_state)\n",
    "            agent.q_learn(state,action,reward,next_state)\n",
    "            state=next_state\n",
    "            step_counter+=1\n",
    "            if done:\n",
    "                break\n",
    "        clear_output(wait=True)\n",
    "        print(f'episode {i} have been done in {step_counter} steps')\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "    m.title('Maze - Solved')\n",
    "\n",
    "    UNIVERSE_SLEEP = 1\n",
    "    while True:\n",
    "        state = m.reset()\n",
    "        state = str(state)\n",
    "        step_counter = 0\n",
    "        while True:\n",
    "            action = agent.certain_action(state)\n",
    "            clear_output(wait=True)\n",
    "            print(f'action : {action}')\n",
    "            print(agent.q_table)\n",
    "            m.render()\n",
    "            next_state,reward,done = m.step(action)\n",
    "            next_state = str(next_state)\n",
    "            state=next_state\n",
    "            step_counter+=1\n",
    "            if done:\n",
    "                break\n",
    "        clear_output(wait=True)\n",
    "        print(f'reach the goal in {step_counter} steps')\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "def run_with_sarsa():\n",
    "    global UNIVERSE_SLEEP\n",
    "    m = Maze()\n",
    "    agent = RL(m.goal_coords(),m.walls_coords())\n",
    "\n",
    "    m.title('Maze - Training')\n",
    "\n",
    "    for i in range(EPISODE):\n",
    "        state = m.reset()\n",
    "        state = str(state)\n",
    "        action = agent.choose_action(state)\n",
    "        step_counter = 0\n",
    "        while True:\n",
    "            clear_output(wait=True)\n",
    "            print(agent.q_table)\n",
    "            m.render()\n",
    "            next_state,reward,done = m.step(action)\n",
    "            next_state = str(next_state)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.sarsa_learn(state,action,reward,next_state,next_action)\n",
    "            state=next_state\n",
    "            action=next_action\n",
    "            step_counter+=1\n",
    "            if done:\n",
    "                break\n",
    "        clear_output(wait=True)\n",
    "        print(f'episode {i} have been done in {step_counter} steps')\n",
    "        # time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "    m.title('Maze - Solved')\n",
    "\n",
    "    UNIVERSE_SLEEP = 1\n",
    "    while True:\n",
    "        state = m.reset()\n",
    "        state = str(state)\n",
    "        action = agent.certain_action(state)\n",
    "        step_counter = 0\n",
    "        while True:\n",
    "            clear_output(wait=True)\n",
    "            print(f'action : {action}')\n",
    "            print(agent.q_table)\n",
    "            m.render()\n",
    "            next_state,reward,done = m.step(action)\n",
    "            next_state = str(next_state)\n",
    "            next_action = agent.certain_action(next_state)\n",
    "            # agent.sarsa_learn(state,action,reward,next_state,next_action)\n",
    "            state=next_state\n",
    "            action=next_action\n",
    "            step_counter+=1\n",
    "            if done:\n",
    "                break\n",
    "        clear_output(wait=True)\n",
    "        print(f'reach the goal in {step_counter} steps')\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "action : UR\n                                      U          R          D          L  \\\n[5.0, 5.0, 55.0, 55.0]        -3.781552  -2.953812 -17.286894  -3.401744   \n[65.0, 5.0, 115.0, 55.0]      -2.717426  -3.339871 -16.802040  -3.044325   \n[125.0, 5.0, 175.0, 55.0]     -2.765859  -2.693846 -15.883770  -3.046122   \n[125.0, 65.0, 175.0, 115.0]   -0.126103  -3.800000   0.585559  -4.876282   \n[65.0, 65.0, 115.0, 115.0]    -0.160000  -2.000000  -0.160000  -4.821617   \n[185.0, 5.0, 235.0, 55.0]     -2.899624  -2.063535 -18.016276  -2.271473   \n[245.0, 5.0, 295.0, 55.0]     -1.941346  -1.626019  -1.504595  -1.580653   \n[5.0, 65.0, 55.0, 115.0]      -0.182977  -7.476321   0.000000  -5.293535   \n[125.0, 125.0, 175.0, 175.0]  -8.687623  -6.671517   0.303074   1.704214   \n[185.0, 125.0, 235.0, 175.0]  -3.790698   0.000000  -0.000837   0.000000   \n[125.0, 185.0, 175.0, 235.0]   0.595330   0.333857  -8.911075 -12.646817   \n[125.0, 245.0, 175.0, 295.0]   0.000000  -1.460076   0.793826  -1.474929   \n[65.0, 185.0, 115.0, 235.0]    2.273222   0.397898  -5.580000   0.585247   \n[65.0, 125.0, 115.0, 175.0]  -11.222465   0.928820 -14.389669   3.030995   \n[185.0, 185.0, 235.0, 235.0] -11.236135   0.052542  -1.819045   3.524086   \n[5.0, 245.0, 55.0, 295.0]      2.917395 -11.127226   4.198683   5.306660   \n[65.0, 305.0, 115.0, 355.0]  -13.315633  11.912237   8.591013   3.552154   \n[5.0, 305.0, 55.0, 355.0]      0.778552  11.825137   0.802657   0.730888   \n[5.0, 185.0, 55.0, 235.0]      3.001877 -10.616060   8.724548   4.265074   \n[5.0, 125.0, 55.0, 175.0]     -4.823099  -0.294808   0.444429   1.288689   \n[245.0, 185.0, 295.0, 235.0]  -0.678913  -0.410069  -2.065417  -0.148911   \n[245.0, 65.0, 295.0, 115.0]   -2.272553  -1.705009   0.510089 -18.285053   \n[185.0, 65.0, 235.0, 115.0]   -0.271157  -0.342915  -5.066466  -5.030622   \n[245.0, 125.0, 295.0, 175.0]  -0.952174  -0.156772  -0.169974 -16.640889   \n[245.0, 245.0, 295.0, 295.0]  -0.033698   0.260441  28.429957   0.000000   \n[185.0, 245.0, 235.0, 295.0]   0.155800  -0.875458   2.400000   0.000000   \n[65.0, 245.0, 115.0, 295.0]   -3.524510  -2.000000   2.141965   0.000000   \n[125.0, 305.0, 175.0, 355.0] -11.052012  13.580637  11.801881   7.947622   \n[245.0, 305.0, 295.0, 355.0]   0.000000   0.000000   0.000000   0.000000   \n[185.0, 305.0, 235.0, 355.0]  -0.578151  20.585682  16.451577  11.255195   \n\n                                     UR         UL         DR         DL  \\\n[5.0, 5.0, 55.0, 55.0]        -1.752866  -3.173857 -16.414761 -16.326322   \n[65.0, 5.0, 115.0, 55.0]      -1.800932  -2.539341 -15.639336 -17.240914   \n[125.0, 5.0, 175.0, 55.0]     -1.629481  -2.793099 -18.820939 -15.447777   \n[125.0, 65.0, 175.0, 115.0]   -0.013360  -0.088554  -2.000000   4.864740   \n[65.0, 65.0, 115.0, 115.0]    -0.115356  -0.384377   0.197518   4.306069   \n[185.0, 5.0, 235.0, 55.0]     -2.028649  -2.943937  -1.377828 -16.398542   \n[245.0, 5.0, 295.0, 55.0]     -1.578067  -1.623484  -1.196253 -18.744260   \n[5.0, 65.0, 55.0, 115.0]      -0.086171  -0.140315   0.852058   4.039991   \n[125.0, 125.0, 175.0, 175.0]  -8.488455  -4.739983   0.088931 -10.201575   \n[185.0, 125.0, 235.0, 175.0]  -0.044019  -2.000000  -0.008928   2.938846   \n[125.0, 185.0, 175.0, 235.0] -11.260794   5.032892  -2.151516  -9.147048   \n[125.0, 245.0, 175.0, 295.0]   0.423357  -3.716285   4.560000  11.590030   \n[65.0, 185.0, 115.0, 235.0]    0.000000   1.223523  -5.421093   0.000000   \n[65.0, 125.0, 115.0, 175.0]  -14.813446 -14.470876   0.669770   6.494836   \n[185.0, 185.0, 235.0, 235.0]   0.529901   0.523569  -0.718091  -7.674575   \n[5.0, 245.0, 55.0, 295.0]    -16.781792   2.894711  11.705319   6.221617   \n[65.0, 305.0, 115.0, 355.0]  -10.691382   4.783606  16.294500   3.912388   \n[5.0, 305.0, 55.0, 355.0]     -6.475654   1.008440   1.196326   0.831455   \n[5.0, 185.0, 55.0, 235.0]      1.950536   2.558908 -12.118558   4.391923   \n[5.0, 125.0, 55.0, 175.0]     -4.988875  -6.557232  -6.623370   6.297975   \n[245.0, 185.0, 295.0, 235.0]   0.021151  -7.990325  -4.769600  -2.939245   \n[245.0, 65.0, 295.0, 115.0]   -0.715803  -1.009789  -1.092768 -16.340978   \n[185.0, 65.0, 235.0, 115.0]   -0.165321  -0.887501  -0.186619   1.333674   \n[245.0, 125.0, 295.0, 175.0]  -0.153750 -16.962125  -0.230094   1.940360   \n[245.0, 245.0, 295.0, 295.0]   0.000000   0.000000   0.000000   0.000000   \n[185.0, 245.0, 235.0, 295.0]   0.000000   0.340425  29.709068   1.822938   \n[65.0, 245.0, 115.0, 295.0]    0.361069   0.587563   1.820139   8.829128   \n[125.0, 305.0, 175.0, 355.0]  -0.275600 -13.506993  21.776436   8.303792   \n[245.0, 305.0, 295.0, 355.0]   0.000000   0.000000   0.000000   0.000000   \n[185.0, 305.0, 235.0, 355.0]  -4.393038  -7.830369  30.000000  12.373009   \n\n                                     NO  \n[5.0, 5.0, 55.0, 55.0]        -2.850005  \n[65.0, 5.0, 115.0, 55.0]      -2.802622  \n[125.0, 5.0, 175.0, 55.0]     -2.949908  \n[125.0, 65.0, 175.0, 115.0]   -2.000000  \n[65.0, 65.0, 115.0, 115.0]    -3.800000  \n[185.0, 5.0, 235.0, 55.0]     -2.096481  \n[245.0, 5.0, 295.0, 55.0]     -2.101740  \n[5.0, 65.0, 55.0, 115.0]      -3.800000  \n[125.0, 125.0, 175.0, 175.0]   0.078728  \n[185.0, 125.0, 235.0, 175.0]  -2.000000  \n[125.0, 185.0, 175.0, 235.0]   0.562268  \n[125.0, 245.0, 175.0, 295.0]  -3.800000  \n[65.0, 185.0, 115.0, 235.0]   -3.800000  \n[65.0, 125.0, 115.0, 175.0]    2.232474  \n[185.0, 185.0, 235.0, 235.0]   0.481223  \n[5.0, 245.0, 55.0, 295.0]      4.733078  \n[65.0, 305.0, 115.0, 355.0]    6.522379  \n[5.0, 305.0, 55.0, 355.0]      0.000000  \n[5.0, 185.0, 55.0, 235.0]      4.552661  \n[5.0, 125.0, 55.0, 175.0]      0.991529  \n[245.0, 185.0, 295.0, 235.0]  -0.255056  \n[245.0, 65.0, 295.0, 115.0]   -1.391289  \n[185.0, 65.0, 235.0, 115.0]   -5.412030  \n[245.0, 125.0, 295.0, 175.0]  -0.700262  \n[245.0, 245.0, 295.0, 295.0]   0.000000  \n[185.0, 245.0, 235.0, 295.0]   0.000000  \n[65.0, 245.0, 115.0, 295.0]   -1.508781  \n[125.0, 305.0, 175.0, 355.0]   9.230144  \n[245.0, 305.0, 295.0, 355.0]   0.000000  \n[185.0, 305.0, 235.0, 355.0]  14.664147  \n"
    },
    {
     "output_type": "error",
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fc126bcb0bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_with_sarsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-942fad291d78>\u001b[0m in \u001b[0;36mrun_with_sarsa\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcertain_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0e70646840d8>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mmovement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/universe/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2467\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[1;32m   2468\u001b[0m                            self.tk.splitlist(\n\u001b[0;32m-> 2469\u001b[0;31m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[1;32m   2470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m         \u001b[0;34m\"\"\"Internal function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "run_with_sarsa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python361064bituniversecondaaf2664f9b75c43c390322f1777d5968c",
   "display_name": "Python 3.6.10 64-bit ('universe': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}